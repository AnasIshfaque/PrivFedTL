{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eec053bb-e362-466c-b05b-1e931c2ca20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import medmnist\n",
    "from medmnist import BreastMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import lpips\n",
    "from torchvision import models\n",
    "from torchvision.models import SqueezeNet1_1_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf17b51-483b-49f3-aad7-1d026653c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse(img1, img2):\n",
    "    return ((img1 - img2) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68357a59-303c-4f9e-b171-bb2090874112",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.array([0.485,0.456,0.406])\n",
    "std = np.array([0.229,0.224,0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9849f68d-1f00-4cac-9f67-0984d458302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BreastMNIST dataset\n",
    "def load_breastmnist(data_flag='train'):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std)\n",
    "    ])\n",
    "    dataset = BreastMNIST(split=data_flag, download=True, transform=transform)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2480496-9ef1-4f0f-bc9a-1c4a483b7b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'BreastMNIST'\n",
    "num_classes = 2\n",
    "lr = 1.0\n",
    "num_dummy = 4\n",
    "Iteration = 50\n",
    "num_exp = 1\n",
    "client_lr = 0.01  # Client learning rate\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70ec7d9f-f7ef-4c39-9492-963ceb3e1179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if use_cuda else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1fe700b5-828d-4338-b3ca-7ed1b6ef83ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dst = load_breastmnist()\n",
    "dataloader = DataLoader(dst, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2807d04a-1d60-4e4a-9411-2b25bd199654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize weights\n",
    "def weights_init(m):\n",
    "    try:\n",
    "        if hasattr(m, \"weight\"):\n",
    "            m.weight.data.uniform_(-0.5, 0.5)\n",
    "    except Exception:\n",
    "        print(f'Warning: failed in weights_init for {m._get_name()}.weight')\n",
    "    try:\n",
    "        if hasattr(m, \"bias\"):\n",
    "            m.bias.data.uniform_(-0.5, 0.5)\n",
    "    except Exception:\n",
    "        print(f'Warning: failed in weights_init for {m._get_name()}.bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e151e0ea-f48d-4cf4-999c-ef4955be9ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model parameters from two consecutive FL rounds\n",
    "round_9_params = torch.load(\"round_9.pth\", map_location=device)\n",
    "round_10_params = torch.load(\"round_10.pth\", map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "368eb8b6-3b00-4a39-813e-84d0daaba7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute estimated gradient\n",
    "estimated_dy_dx = []\n",
    "for key in round_9_params.keys():\n",
    "    grad_estimate = (round_10_params[key] - round_9_params[key]) / client_lr\n",
    "    estimated_dy_dx.append(grad_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "656be389-3a09-4c49-a4ec-7ea1b71b2328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1/3 experiment\n",
      "Iteration 1: Loss=308246715826176.000000, MSE=2.188584\n",
      "Iteration 6: Loss=4875103502336.000000, MSE=2.116808\n",
      "Iteration 11: Loss=4875103502336.000000, MSE=2.116808\n",
      "Iteration 16: Loss=4875103502336.000000, MSE=2.116808\n",
      "Iteration 21: Loss=4875103502336.000000, MSE=2.116808\n",
      "Iteration 26: Loss=4875103502336.000000, MSE=2.116808\n",
      "Iteration 31: Loss=4875103502336.000000, MSE=2.116808\n",
      "Iteration 36: Loss=4875103502336.000000, MSE=2.116808\n",
      "Iteration 41: Loss=4875103502336.000000, MSE=2.116808\n",
      "Iteration 46: Loss=4875103502336.000000, MSE=2.116808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1007793..1.1236603].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1007793..1.1236603].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1007793..1.1236603].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1007793..1.1236603].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1007793..1.1236603].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1007793..1.1236603].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1007793..1.1236603].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1007793..1.1236603].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1007793..1.1236603].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1007793..1.1236603].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-2.1007793..1.1236603].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 50: Loss=4875103502336.000000, MSE=2.116808\n",
      "Experiment 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (4, 3, 28, 28) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m     axes[i]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miter_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     66\u001b[0m     axes[i]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[43maxes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgray\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m axes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     70\u001b[0m axes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moff\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/PrivFedTL/anasenv/lib/python3.10/site-packages/matplotlib/__init__.py:1521\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[43m            \u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1523\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1524\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcbook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1526\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1527\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1528\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/PrivFedTL/anasenv/lib/python3.10/site-packages/matplotlib/axes/_axes.py:5945\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5943\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m-> 5945\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5946\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5948\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/PrivFedTL/anasenv/lib/python3.10/site-packages/matplotlib/image.py:675\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    674\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_normalize_image_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/PrivFedTL/anasenv/lib/python3.10/site-packages/matplotlib/image.py:643\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    641\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    645\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    647\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    649\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (4, 3, 28, 28) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx_net in range(num_exp):\n",
    "    net = models.squeezenet1_1(weights=SqueezeNet1_1_Weights.DEFAULT)\n",
    "\n",
    "    # Freezing previous layers\n",
    "    # for param in net.features.parameters():\n",
    "    #     param.requires_grad = False\n",
    "\n",
    "    # Modifying the last layer to match desired output class\n",
    "    num_classes = 2\n",
    "    net.classifier = nn.Sequential(\n",
    "        nn.Conv2d(512, num_classes, kernel_size=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.AdaptiveAvgPool2d((1, 1))\n",
    "    )\n",
    "    \n",
    "    net.apply(weights_init)\n",
    "    net.to(device)\n",
    "\n",
    "    print(f'Running {idx_net+1}/{num_exp} experiment')\n",
    "\n",
    "    for data, target in dataloader:\n",
    "        data, target = data.to(device), target.to(device).long().view(-1)\n",
    "        break  # Get a single batch\n",
    "\n",
    "    # Initialize dummy data and labels\n",
    "    dummy_data = torch.randn_like(data, device=device, requires_grad=True)\n",
    "    dummy_label = torch.randint(0, num_classes, (data.shape[0],), device=device)  # Fixed outside closure\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.LBFGS([dummy_data], lr=lr)\n",
    "\n",
    "    history = []\n",
    "    history_iters = []\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        preds = net(dummy_data)\n",
    "        criterion = nn.CrossEntropyLoss().to(device)\n",
    "        dummy_loss = criterion(preds, dummy_label)\n",
    "        dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
    "        \n",
    "        # Compute gradient matching loss\n",
    "        grad_diff = sum(((gx - gy) ** 2).sum() for gx, gy in zip(dummy_dy_dx, estimated_dy_dx))\n",
    "        grad_diff.backward(retain_graph=True)  # Retain graph to prevent multiple backward calls\n",
    "        return grad_diff\n",
    "        \n",
    "    for iters in range(Iteration):\n",
    "        optimizer.step(closure)\n",
    "\n",
    "        # Compute loss, MSE, and SSIM\n",
    "        with torch.no_grad():\n",
    "            mse_val = compute_mse(dummy_data.cpu().squeeze().numpy(), data.cpu().squeeze().numpy())\n",
    "            # ssim_val = ssim(dummy_data.cpu().squeeze().numpy(), data.cpu().squeeze().numpy(), data_range=1, win_size=win_size)\n",
    "        \n",
    "        if iters % (Iteration // 10) == 0 or (iters+1) == Iteration:\n",
    "            print(f\"Iteration {iters+1}: Loss={closure().item():.6f}, MSE={mse_val:.6f}\")\n",
    "            history.append(dummy_data.detach().cpu().squeeze().numpy())\n",
    "            history_iters.append(iters)\n",
    "    \n",
    "    print(f'Experiment {idx_net+1}')\n",
    "    fig, axes = plt.subplots(1, len(history) + 1, figsize=(15, 3))\n",
    "    for i, (img, iter_num) in enumerate(zip(history, history_iters)):\n",
    "        # axes[i].imshow(img, cmap='gray')\n",
    "        axes[-1].imshow(data[0].permute(1, 2, 0).cpu().numpy())  # Use first sample, RGB\n",
    "        axes[i].set_title(f\"Iter {iter_num}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    axes[-1].imshow(data.cpu().squeeze(), cmap='gray')\n",
    "    axes[-1].set_title(\"Original\")\n",
    "    axes[-1].axis('off')\n",
    "    \n",
    "    plt.savefig(f\"reconstruction_progress_exp{idx_net+1}.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8c24a5-16e3-4102-9248-0d125d35094f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b9760a-d220-434a-a81a-78fc35d0e7b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m estimated_gradients:\n\u001b[1;32m     57\u001b[0m         grad_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_fn(param\u001b[38;5;241m.\u001b[39mgrad, estimated_gradients[name])\n\u001b[0;32m---> 59\u001b[0m \u001b[43mgrad_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m()\n\u001b[1;32m     60\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from medmnist import BreastMNIST\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Constants\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "client_lr = 0.01  # Client learning rate\n",
    "BATCH_SIZE = 4  # Batch size used by the client\n",
    "num_iters = 50  # Number of optimization iterations\n",
    "\n",
    "# Load model parameters from two FL rounds\n",
    "round_9_params = torch.load(\"client0_9th_round.pth\", map_location=device)\n",
    "round_10_params = torch.load(\"client0_10th_round.pth\", map_location=device)\n",
    "\n",
    "# Compute estimated gradient (difference divided by learning rate)\n",
    "estimated_gradients = {key: (round_10_params[key] - round_9_params[key]) / client_lr for key in round_9_params}\n",
    "\n",
    "# Load BreastMNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "dataset = BreastMNIST(split=\"train\", download=True, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Initialize dummy input\n",
    "dummy_input = torch.randn((BATCH_SIZE, 3, 28, 28), device=device, requires_grad=True)\n",
    "optimizer = optim.Adam([dummy_input], lr=0.1)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Define a simple dummy model to approximate the client's model behavior\n",
    "dummy_model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, 3, padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(16, 2, 3, padding=1)\n",
    ").to(device)\n",
    "\n",
    "# Optimization loop to recover the input\n",
    "for i in range(num_iters):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass on dummy model\n",
    "    dummy_output = dummy_model(dummy_input)\n",
    "    dummy_output.sum().backward()\n",
    "    \n",
    "    # Compute gradient loss (matching keys from estimated gradients)\n",
    "    grad_loss = 0\n",
    "    for name, param in dummy_model.named_parameters():\n",
    "        if name in estimated_gradients:\n",
    "            grad_loss += loss_fn(param.grad, estimated_gradients[name])\n",
    "    \n",
    "    grad_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Iteration {i}: Loss = {grad_loss.item()}\")\n",
    "\n",
    "# Evaluate reconstruction\n",
    "original_images, _ = next(iter(dataloader))\n",
    "original_images = original_images.to(device)\n",
    "\n",
    "# Compute MSE\n",
    "mse_loss = ((original_images - dummy_input.detach()) ** 2).mean().item()\n",
    "\n",
    "# Compute SSIM\n",
    "def compute_ssim(img1, img2):\n",
    "    img1 = img1.squeeze().cpu().numpy()\n",
    "    img2 = img2.squeeze().cpu().numpy()\n",
    "    return ssim(img1, img2, data_range=img1.max() - img1.min(), multichannel=True)\n",
    "\n",
    "ssim_value = np.mean([compute_ssim(original_images[i], dummy_input[i].detach()) for i in range(BATCH_SIZE)])\n",
    "\n",
    "print(f\"MSE: {mse_loss}, SSIM: {ssim_value}\")\n",
    "\n",
    "# Plot original and reconstructed images\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(original_images[0].permute(1, 2, 0).cpu().numpy())\n",
    "ax[0].set_title(\"Original\")\n",
    "ax[1].imshow(dummy_input[0].detach().permute(1, 2, 0).cpu().numpy())\n",
    "ax[1].set_title(\"Reconstructed\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e811247-7925-46e4-845e-ae8dd33b9a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
