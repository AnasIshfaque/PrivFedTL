{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedMNIST Federated Transfer learning with HE SqueezeNet Server Side\n",
    "This code is the server part of CIFAR10 federated mobilenet for **multi** client and a server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds = 10 # number of communication round (server-client-server)\n",
    "local_epoch = 1 # number of epoch in per client per round\n",
    "users = 4 # number of clients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "\n",
    "import socket\n",
    "import struct\n",
    "import pickle\n",
    "import sys\n",
    "import zlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms,models\n",
    "from torchvision.models import SqueezeNet1_1_Weights\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from threading import Thread\n",
    "from threading import Lock\n",
    "\n",
    "import tenseal as ts\n",
    "\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import Accuracy, Precision, Recall, F1Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading public context object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../playground/Developing H.E for FL/public_context.pkl', 'rb') as inp:\n",
    "    public_context_bin = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_context = ts.context_from(public_context_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../../playground/Developing H.E for FL/shared_context.pkl', 'rb') as inp:\n",
    "    shared_context_bin = pickle.load(inp)\n",
    "shared_context = ts.context_from(shared_context_bin)\n",
    "sk = shared_context.secret_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch layer modules for *Conv1D* Network\n",
    "\n",
    "\n",
    "\n",
    "### `Conv1d` layer\n",
    "- `torch.nn.Conv1d(in_channels, out_channels, kernel_size)`\n",
    "\n",
    "### `MaxPool1d` layer\n",
    "- `torch.nn.MaxPool1d(kernel_size, stride=None)`\n",
    "- Parameter `stride` follows `kernel_size`.\n",
    "\n",
    "### `ReLU` layer\n",
    "- `torch.nn.ReLU()`\n",
    "\n",
    "### `Linear` layer\n",
    "- `torch.nn.Linear(in_features, out_features, bias=True)`\n",
    "\n",
    "### `Softmax` layer\n",
    "- `torch.nn.Softmax(dim=None)`\n",
    "- Parameter `dim` is usually set to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_model = models.squeezenet1_1(weights=SqueezeNet1_1_Weights.DEFAULT)\n",
    "sq_model.to(device)\n",
    "lr = 0.01\n",
    "optimizer = optim.SGD(sq_model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "step_lr_scheduler = lr_scheduler.StepLR(optimizer,step_size=7,gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freezing previous layers\n",
    "for param in sq_model.features.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifying the last layer to match desired output class\n",
    "num_classes = 2 # CHANGE this to the number of classes of the dataset\n",
    "in_ftrs = sq_model.classifier[1].in_channels\n",
    "features = list(sq_model.classifier.children())[:-3] # Remove last 3 layers\n",
    "features.extend([nn.Conv2d(in_ftrs, num_classes, kernel_size=1)]) # Add\n",
    "features.extend([nn.ReLU(inplace=True)]) # Add\n",
    "features.extend([nn.AdaptiveAvgPool2d(output_size=(1,1))]) # Add\n",
    "sq_model.classifier = nn.Sequential(*features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 'malignant', '1': 'normal, benign'}\n",
      "Using downloaded and verified file: /home/anas/.medmnist/breastmnist_128.npz\n",
      "Using downloaded and verified file: /home/anas/.medmnist/breastmnist_128.npz\n"
     ]
    }
   ],
   "source": [
    "sets = ['train','test']\n",
    "mean = np.array([0.485,0.456,0.406])\n",
    "std = np.array([0.229,0.224,0.225])\n",
    "data_transforms = {\n",
    "    'train':transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std)\n",
    "    ]),\n",
    "    'test':transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std)\n",
    "    ])\n",
    "}\n",
    "dataset_name = 'pneumoniamnist'\n",
    "info = INFO[dataset_name]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])\n",
    "class_labels = info['label']\n",
    "print(info['label'])\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "sets = ['train','test']\n",
    "image_datasets = {x:DataClass(split=x, transform=data_transforms[x], download=True, size=128)\n",
    "                for x in ['train','test']}\n",
    "\n",
    "dataloaders = {'train': torch.utils.data.DataLoader(image_datasets['train'],batch_size=BATCH_SIZE,\n",
    "                                            shuffle=True,num_workers=0),\n",
    "               'test': torch.utils.data.DataLoader(image_datasets['test'],batch_size=2*BATCH_SIZE,\n",
    "                                            shuffle=True,num_workers=0)\n",
    "              }\n",
    "\n",
    "# train_total_batch is dataset_sizes['train'] and test_batch is dataset_sizes['test']\n",
    "dataset_sizes = {x:len(image_datasets[x]) for x in ['train','test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(preds, labels):\n",
    "    # metric = torchmetrics.classification.Accuracy(task=\"multiclass\", num_classes=n_classes)\n",
    "    \n",
    "    accuracy_metric = Accuracy(task=\"multiclass\", num_classes=n_classes)\n",
    "    accuracy_metric(preds.cpu(), labels.cpu())\n",
    "    print(f'Accuracy: {accuracy_metric.compute():.4f}')\n",
    "\n",
    "    #calculate precision\n",
    "    precision = Precision(task=\"multiclass\", average='macro', num_classes=n_classes)\n",
    "    precision(preds.cpu(), labels.cpu())\n",
    "    print(f'Precision: {precision.compute():.4f}')\n",
    "\n",
    "    #calculate recall\n",
    "    recall = Recall(task=\"multiclass\", average='macro', num_classes=n_classes)\n",
    "    recall(preds.cpu(), labels.cpu())\n",
    "    print(f'Recall: {recall.compute():.4f}')\n",
    "\n",
    "    #calculate f1 score\n",
    "    f1 = F1Score(task=\"multiclass\", average='macro', num_classes=n_classes)\n",
    "    f1(preds.cpu(), labels.cpu())\n",
    "    print(f'F1: {f1.compute():.4f}')\n",
    "\n",
    "    #calculate confusion matrix\n",
    "    cm = torchmetrics.functional.confusion_matrix(preds.cpu(), labels.cpu(), num_classes=n_classes, task=\"multiclass\")\n",
    "    print(f'Confusion Matrix: \\n{cm}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy and loss\n",
    "def calculate_performance(model, dataloader, criterion, extra_metrics=False):\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _,preds = torch.max(outputs,1)\n",
    "        labels = labels.squeeze().long()\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    average_loss = total_loss / total_samples\n",
    "\n",
    "    if(extra_metrics):\n",
    "        eval_model(preds, labels)\n",
    "\n",
    "    return accuracy, average_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "clientsoclist = [0]*users\n",
    "\n",
    "start_time = 0\n",
    "weight_count = 0\n",
    "\n",
    "last_layer_list = [sq_model.state_dict()['classifier.1.weight'], sq_model.state_dict()['classifier.1.bias']]\n",
    "global_weights = copy.deepcopy(sq_model.state_dict())\n",
    "\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "\n",
    "datasetsize = [0]*users\n",
    "weights_list = [0]*users\n",
    "\n",
    "total_aggr_time = 0\n",
    "total_send_time = 0\n",
    "total_recv_time = 0\n",
    "# total_compress_time = 0\n",
    "# total_decompress_time = 0\n",
    "\n",
    "lock = Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_model(target_list):\n",
    "    global global_weights\n",
    "    global sq_model\n",
    "    decrypted_lll = []\n",
    "    for param in target_list:\n",
    "        decrypted_lll.append(torch.tensor(param.decrypt(sk).tolist()))\n",
    "        \n",
    "    global_weights['classifier.1.weight'] = decrypted_lll[0]\n",
    "    global_weights['classifier.1.bias'] = decrypted_lll[1]\n",
    "    \n",
    "    sq_model.load_state_dict(global_weights)\n",
    "    sq_model.eval()\n",
    "    sq_model = sq_model.to(device)\n",
    "    return sq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comunication overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sendsize_list = []\n",
    "total_receivesize_list = []\n",
    "\n",
    "client_sendsize_list = [[] for i in range(users)]\n",
    "client_receivesize_list = [[] for i in range(users)]\n",
    "\n",
    "train_sendsize_list = [] \n",
    "train_receivesize_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Socket initialization\n",
    "### Set host address and port number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required socket functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_msg(sock, msg, serialize=True):\n",
    "    global total_send_time\n",
    "    # global total_compress_time\n",
    "    send_start = time.time()\n",
    "    \n",
    "    # prefix each message with a 4-byte length in network byte order\n",
    "    if serialize:\n",
    "        msg = msg.serialize()\n",
    "    else:        \n",
    "        msg = pickle.dumps(msg)\n",
    "        \n",
    "    # compress_start = time.time()\n",
    "    # msg = zlib.compress(msg)\n",
    "    # compress_end = time.time()\n",
    "    # total_compress_time += (compress_end - compress_start)\n",
    "    \n",
    "    l_send = len(msg)\n",
    "    msg = struct.pack('>I', l_send) + msg\n",
    "    sock.sendall(msg)\n",
    "    \n",
    "    send_end = time.time()\n",
    "    total_send_time += (send_end - send_start)\n",
    "    \n",
    "    return l_send\n",
    "\n",
    "def recv_msg(sock, deserialize=True):\n",
    "    global total_recv_time\n",
    "    # global total_decompress_time\n",
    "    \n",
    "    recv_start = time.time()\n",
    "    # read message length and unpack it into an integer\n",
    "    raw_msglen = recvall(sock, 4)\n",
    "    if not raw_msglen:\n",
    "        return None\n",
    "    msglen = struct.unpack('>I', raw_msglen)[0]\n",
    "    # read the message data\n",
    "    msg =  recvall(sock, msglen)\n",
    "    \n",
    "    # decompress_start = time.time()\n",
    "    # msg = zlib.decompress(msg)\n",
    "    # decompress_end = time.time()\n",
    "    # total_decompress_time += (decompress_end - decompress_start)\n",
    "    \n",
    "    if deserialize:\n",
    "        msg = ts.ckks_tensor_from(public_context, msg)\n",
    "    else:\n",
    "        msg = pickle.loads(msg)\n",
    "        \n",
    "    recv_end = time.time()\n",
    "    total_recv_time += (recv_end - recv_start)\n",
    "    \n",
    "    return msg, msglen\n",
    "\n",
    "def recvall(sock, n):\n",
    "    # helper function to receive n bytes or return None if EOF is hit\n",
    "    data = b''\n",
    "    while len(data) < n:\n",
    "        packet = sock.recv(n - len(data))\n",
    "        if not packet:\n",
    "            return None\n",
    "        data += packet\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def average_weights(w, datasize):\n",
    "    \"\"\"\n",
    "    Returns the average of the weights.\n",
    "    \"\"\"   \n",
    "    for i, data in enumerate(datasize):\n",
    "        for j in range(len(w[i])):\n",
    "            w[i][j] *= float(data)\n",
    "    \n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "\n",
    "    for i in range(len(w_avg)):\n",
    "        for j in range(1, len(w)):\n",
    "            w_avg[i] += w[j][i]\n",
    "            #eval add\n",
    "        w_avg[i] = w_avg[i] * (1/float(sum(datasize)))\n",
    "\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thread define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receive users before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_thread(func, num_user):\n",
    "    global clientsoclist\n",
    "    global start_time\n",
    "    \n",
    "    thrs = []\n",
    "    for i in range(num_user):\n",
    "        conn, addr = s.accept()\n",
    "        print('Conntected with', addr)\n",
    "        # append client socket on list\n",
    "        clientsoclist[i] = conn\n",
    "        args = (i, num_user, conn)\n",
    "        thread = Thread(target=func, args=args)\n",
    "        thrs.append(thread)\n",
    "        thread.start()\n",
    "    print(\"timmer start!\")\n",
    "    start_time = time.time()    # store start time\n",
    "    for thread in thrs:\n",
    "        thread.join()\n",
    "    end_time = time.time()  # store end time\n",
    "    print(\"TrainingTime: {} sec\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def receive(userid, num_users, conn): #thread for receive clients\n",
    "    global weight_count\n",
    "    \n",
    "    global datasetsize\n",
    "\n",
    "\n",
    "    msg = {\n",
    "        'rounds': rounds,\n",
    "        'client_id': userid,\n",
    "        'local_epoch': local_epoch,\n",
    "        'last_layer_list_len':len(last_layer_list)\n",
    "    }\n",
    "\n",
    "    datasize = send_msg(conn, msg, False)    #send epoch\n",
    "    total_sendsize_list.append(datasize)\n",
    "    client_sendsize_list[userid].append(datasize)\n",
    "\n",
    "    train_dataset_size, datasize = recv_msg(conn, False)    # get total_batch of train dataset\n",
    "    total_receivesize_list.append(datasize)\n",
    "    client_receivesize_list[userid].append(datasize)\n",
    "    \n",
    "    \n",
    "    with lock:\n",
    "        datasetsize[userid] = train_dataset_size\n",
    "        weight_count += 1\n",
    "    \n",
    "    train(userid, train_dataset_size, num_users, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(userid, train_dataset_size, num_users, client_conn):\n",
    "    global weights_list\n",
    "    global global_weights\n",
    "    global last_layer_list\n",
    "    global weight_count\n",
    "    global sq_model\n",
    "    global val_acc\n",
    "    global total_aggr_time\n",
    "    global train_acc_list\n",
    "    global val_acc_list\n",
    "    global train_loss_list\n",
    "    global val_loss_list\n",
    "    \n",
    "    for r in range(rounds):\n",
    "        with lock:\n",
    "            if weight_count == num_users:\n",
    "                for i, conn in enumerate(clientsoclist):\n",
    "                    print(f'sending global weights to client {i}...')\n",
    "                    if r == 0:\n",
    "                        datasize = send_msg(conn, last_layer_list, False) # sending last layer parameters only\n",
    "                    else:\n",
    "                        datasize = 0\n",
    "                        for param in last_layer_list:\n",
    "                            datasize += send_msg(conn, param)\n",
    "                    print(f'global weights to client {i} is sent!')        \n",
    "                    total_sendsize_list.append(datasize)\n",
    "                    client_sendsize_list[i].append(datasize)\n",
    "                    train_sendsize_list.append(datasize)\n",
    "                    weight_count = 0\n",
    "\n",
    "        client_weights = [] # client_weights refers to the last layer weights of the client\n",
    "        print(f'receiving client {userid} weights...')\n",
    "        for i in range(len(last_layer_list)):\n",
    "            client_weight, datasize = recv_msg(client_conn) \n",
    "            client_weights.append(client_weight)\n",
    "            \n",
    "            total_receivesize_list.append(datasize)\n",
    "            client_receivesize_list[userid].append(datasize)\n",
    "            train_receivesize_list.append(datasize)\n",
    "        print(f'client {userid} weights received!')\n",
    "\n",
    "        weights_list[userid] = client_weights\n",
    "        print(\"User\" + str(userid) + \"'s Round \" + str(r + 1) +  \" is done\")\n",
    "        with lock:\n",
    "            weight_count += 1\n",
    "            if weight_count == num_users:\n",
    "                aggr_start = time.time()\n",
    "                #average\n",
    "                print(f'aggregation for round {r+1} starts...')\n",
    "                last_layer_list = average_weights(weights_list, datasetsize) # find the average last layer weights\n",
    "                #tracking the global model performance per round\n",
    "                copy_lll = copy.deepcopy(last_layer_list)\n",
    "                sq_model = get_global_model(copy_lll)\n",
    "                train_acc, train_loss = calculate_performance(sq_model, dataloaders['train'], criterion)\n",
    "                val_acc, val_loss = calculate_performance(sq_model, dataloaders['test'], criterion)\n",
    "                train_acc_list.append(train_acc)\n",
    "                val_acc_list.append(val_acc)\n",
    "                train_loss_list.append(train_loss)\n",
    "                val_loss_list.append(val_loss)\n",
    "                \n",
    "                aggr_end = time.time()\n",
    "                total_aggr_time += (aggr_end - aggr_start)\n",
    "                print(f'aggregation for round {r+1} ends')\n",
    "                \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192.168.0.144\n"
     ]
    }
   ],
   "source": [
    "host = socket.gethostbyname(socket.gethostname())\n",
    "port = 10080\n",
    "print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = socket.socket()\n",
    "s.bind((host, port))\n",
    "s.listen(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the server socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conntected with ('192.168.0.114', 50368)\n",
      "receiving client 0 weights...\n",
      "Conntected with ('192.168.0.160', 35746)\n",
      "receiving client 1 weights...\n",
      "Conntected with ('192.168.0.148', 35404)\n",
      "receiving client 2 weights...\n",
      "Conntected with ('192.168.0.137', 59654)\n",
      "timmer start!\n",
      "sending global weights to client 0...\n",
      "global weights to client 0 is sent!\n",
      "sending global weights to client 1...\n",
      "global weights to client 1 is sent!\n",
      "sending global weights to client 2...\n",
      "global weights to client 2 is sent!\n",
      "sending global weights to client 3...\n",
      "global weights to client 3 is sent!\n",
      "receiving client 3 weights...\n",
      "client 1 weights received!\n",
      "User1's Round 1 is done\n",
      "receiving client 1 weights...\n",
      "client 2 weights received!\n",
      "User2's Round 1 is done\n",
      "receiving client 2 weights...\n",
      "client 3 weights received!\n",
      "User3's Round 1 is done\n",
      "receiving client 3 weights...\n",
      "client 0 weights received!\n",
      "User0's Round 1 is done\n",
      "aggregation for round 1 starts...\n",
      "aggregation for round 1 ends\n",
      "sending global weights to client 0...\n",
      "global weights to client 0 is sent!\n",
      "sending global weights to client 1...\n",
      "global weights to client 1 is sent!\n",
      "sending global weights to client 2...\n",
      "global weights to client 2 is sent!\n",
      "sending global weights to client 3...\n",
      "client 1 weights received!\n",
      "User1's Round 2 is done\n",
      "client 2 weights received!\n",
      "User2's Round 2 is done\n",
      "global weights to client 3 is sent!\n",
      "receiving client 0 weights...\n",
      "receiving client 1 weights...\n",
      "receiving client 2 weights...\n",
      "client 0 weights received!\n",
      "User0's Round 2 is done\n",
      "receiving client 0 weights...\n",
      "client 3 weights received!\n",
      "User3's Round 2 is done\n",
      "aggregation for round 2 starts...\n",
      "aggregation for round 2 ends\n",
      "sending global weights to client 0...\n",
      "global weights to client 0 is sent!\n",
      "sending global weights to client 1...\n",
      "global weights to client 1 is sent!\n",
      "sending global weights to client 2...\n",
      "client 0 weights received!\n",
      "User0's Round 3 is done\n",
      "global weights to client 2 is sent!\n",
      "sending global weights to client 3...\n",
      "client 1 weights received!\n",
      "User1's Round 3 is done\n",
      "global weights to client 3 is sent!\n",
      "receiving client 3 weights...\n",
      "receiving client 0 weights...\n",
      "receiving client 1 weights...\n",
      "client 2 weights received!\n",
      "User2's Round 3 is done\n",
      "receiving client 2 weights...\n",
      "client 3 weights received!\n",
      "User3's Round 3 is done\n",
      "aggregation for round 3 starts...\n",
      "aggregation for round 3 ends\n",
      "sending global weights to client 0...\n",
      "global weights to client 0 is sent!\n",
      "sending global weights to client 1...\n",
      "global weights to client 1 is sent!\n",
      "sending global weights to client 2...\n",
      "client 0 weights received!\n",
      "User0's Round 4 is done\n",
      "global weights to client 2 is sent!\n",
      "sending global weights to client 3...\n",
      "client 1 weights received!\n",
      "User1's Round 4 is done\n",
      "global weights to client 3 is sent!\n",
      "receiving client 3 weights...\n",
      "receiving client 0 weights...\n",
      "receiving client 1 weights...\n",
      "client 2 weights received!\n",
      "User2's Round 4 is done\n",
      "receiving client 2 weights...\n",
      "client 3 weights received!\n",
      "User3's Round 4 is done\n",
      "aggregation for round 4 starts...\n",
      "aggregation for round 4 ends\n",
      "sending global weights to client 0...\n",
      "global weights to client 0 is sent!\n",
      "sending global weights to client 1...\n",
      "global weights to client 1 is sent!\n",
      "sending global weights to client 2...\n",
      "global weights to client 2 is sent!\n",
      "sending global weights to client 3...\n",
      "client 0 weights received!\n",
      "User0's Round 5 is done\n",
      "global weights to client 3 is sent!\n",
      "receiving client 3 weights...\n",
      "receiving client 0 weights...\n",
      "client 1 weights received!\n",
      "User1's Round 5 is done\n",
      "receiving client 1 weights...\n",
      "client 2 weights received!\n",
      "User2's Round 5 is done\n",
      "receiving client 2 weights...\n",
      "client 3 weights received!\n",
      "User3's Round 5 is done\n",
      "aggregation for round 5 starts...\n",
      "aggregation for round 5 ends\n",
      "sending global weights to client 0...\n",
      "global weights to client 0 is sent!\n",
      "sending global weights to client 1...\n",
      "global weights to client 1 is sent!\n",
      "sending global weights to client 2...\n",
      "global weights to client 2 is sent!\n",
      "sending global weights to client 3...\n",
      "client 0 weights received!\n",
      "User0's Round 6 is done\n",
      "global weights to client 3 is sent!\n",
      "receiving client 3 weights...\n",
      "receiving client 0 weights...\n",
      "client 1 weights received!\n",
      "User1's Round 6 is done\n",
      "receiving client 1 weights...\n",
      "client 2 weights received!\n",
      "User2's Round 6 is done\n",
      "receiving client 2 weights...\n",
      "client 3 weights received!\n",
      "User3's Round 6 is done\n",
      "aggregation for round 6 starts...\n",
      "aggregation for round 6 ends\n",
      "sending global weights to client 0...\n",
      "global weights to client 0 is sent!\n",
      "sending global weights to client 1...\n",
      "global weights to client 1 is sent!\n",
      "sending global weights to client 2...\n",
      "global weights to client 2 is sent!\n",
      "sending global weights to client 3...\n",
      "client 0 weights received!\n",
      "User0's Round 7 is done\n",
      "global weights to client 3 is sent!\n",
      "receiving client 3 weights...\n",
      "receiving client 0 weights...\n",
      "client 1 weights received!\n",
      "User1's Round 7 is done\n",
      "receiving client 1 weights...\n",
      "client 2 weights received!\n",
      "User2's Round 7 is done\n",
      "receiving client 2 weights...\n",
      "client 3 weights received!\n",
      "User3's Round 7 is done\n",
      "aggregation for round 7 starts...\n",
      "aggregation for round 7 ends\n",
      "sending global weights to client 0...\n",
      "global weights to client 0 is sent!\n",
      "sending global weights to client 1...\n",
      "global weights to client 1 is sent!\n",
      "sending global weights to client 2...\n",
      "global weights to client 2 is sent!\n",
      "sending global weights to client 3...\n",
      "client 0 weights received!\n",
      "User0's Round 8 is done\n",
      "global weights to client 3 is sent!\n",
      "receiving client 3 weights...\n",
      "receiving client 0 weights...\n",
      "client 1 weights received!\n",
      "User1's Round 8 is done\n",
      "receiving client 1 weights...\n",
      "client 2 weights received!\n",
      "User2's Round 8 is done\n",
      "receiving client 2 weights...\n",
      "client 3 weights received!\n",
      "User3's Round 8 is done\n",
      "aggregation for round 8 starts...\n",
      "aggregation for round 8 ends\n",
      "sending global weights to client 0...\n",
      "global weights to client 0 is sent!\n",
      "sending global weights to client 1...\n",
      "global weights to client 1 is sent!\n",
      "sending global weights to client 2...\n",
      "global weights to client 2 is sent!\n",
      "sending global weights to client 3...\n",
      "client 0 weights received!\n",
      "User0's Round 9 is done\n",
      "global weights to client 3 is sent!\n",
      "receiving client 3 weights...\n",
      "receiving client 0 weights...\n",
      "client 1 weights received!\n",
      "User1's Round 9 is done\n",
      "receiving client 1 weights...\n",
      "client 2 weights received!\n",
      "User2's Round 9 is done\n",
      "receiving client 2 weights...\n",
      "client 3 weights received!\n",
      "User3's Round 9 is done\n",
      "aggregation for round 9 starts...\n",
      "aggregation for round 9 ends\n",
      "sending global weights to client 0...\n",
      "global weights to client 0 is sent!\n",
      "sending global weights to client 1...\n",
      "global weights to client 1 is sent!\n",
      "sending global weights to client 2...\n",
      "global weights to client 2 is sent!\n",
      "sending global weights to client 3...\n",
      "client 0 weights received!\n",
      "User0's Round 10 is done\n",
      "global weights to client 3 is sent!\n",
      "receiving client 3 weights...\n",
      "client 1 weights received!\n",
      "User1's Round 10 is done\n",
      "client 2 weights received!\n",
      "User2's Round 10 is done\n",
      "client 3 weights received!\n",
      "User3's Round 10 is done\n",
      "aggregation for round 10 starts...\n",
      "aggregation for round 10 ends\n",
      "TrainingTime: 29757.189023971558 sec\n"
     ]
    }
   ],
   "source": [
    "run_thread(receive, users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingTime: 29757.192383527756 sec\n",
      "Total aggrigation time: 138.1046826839447 sec\n",
      "Total send time: 20880.924551010132 sec\n",
      "Total recv time: 90749.49344801903 sec\n",
      "Total communication time: 111630.41799902916 sec\n"
     ]
    }
   ],
   "source": [
    "end_time = time.time()  # store end time\n",
    "print(\"TrainingTime: {} sec\".format(end_time - start_time))\n",
    "print(\"Total aggrigation time: {} sec\".format(total_aggr_time))\n",
    "# print(\"Total compression time: {} sec\".format(total_compress_time))\n",
    "# print(\"Total decompression time: {} sec\".format(total_decompress_time))\n",
    "print(\"Total send time: {} sec\".format(total_send_time))\n",
    "print(\"Total recv time: {} sec\".format(total_recv_time))\n",
    "print(\"Total communication time: {} sec\".format(total_send_time+total_recv_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print all of communication overhead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---total_sendsize_list---\n",
      "total_sendsize size: 5279824104 bytes\n",
      "number of total_send:  44\n",
      "\n",
      "\n",
      "---total_receivesize_list---\n",
      "total receive sizes: 11341660641 bytes\n",
      "number of total receive:  84\n",
      "\n",
      "\n",
      "---client_sendsize_list(user0)---\n",
      "total client_sendsizes(user0): 1319956026 bytes\n",
      "number of client_send(user0):  11\n",
      "\n",
      "\n",
      "---client_receivesize_list(user0)---\n",
      "total client_receive sizes(user0): 2835411722 bytes\n",
      "number of client_send(user0):  21\n",
      "\n",
      "\n",
      "---client_sendsize_list(user1)---\n",
      "total client_sendsizes(user1): 1319956026 bytes\n",
      "number of client_send(user1):  11\n",
      "\n",
      "\n",
      "---client_receivesize_list(user1)---\n",
      "total client_receive sizes(user1): 2835428210 bytes\n",
      "number of client_send(user1):  21\n",
      "\n",
      "\n",
      "---client_sendsize_list(user2)---\n",
      "total client_sendsizes(user2): 1319956026 bytes\n",
      "number of client_send(user2):  11\n",
      "\n",
      "\n",
      "---client_receivesize_list(user2)---\n",
      "total client_receive sizes(user2): 2835410225 bytes\n",
      "number of client_send(user2):  21\n",
      "\n",
      "\n",
      "---client_sendsize_list(user3)---\n",
      "total client_sendsizes(user3): 1319956026 bytes\n",
      "number of client_send(user3):  11\n",
      "\n",
      "\n",
      "---client_receivesize_list(user3)---\n",
      "total client_receive sizes(user3): 2835410484 bytes\n",
      "number of client_send(user3):  21\n",
      "\n",
      "\n",
      "---train_sendsize_list---\n",
      "total train_sendsizes: 5279823780 bytes\n",
      "number of train_send:  40\n",
      "\n",
      "\n",
      "---train_receivesize_list---\n",
      "total train_receivesizes: 11341660621 bytes\n",
      "number of train_receive:  80\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# def commmunication_overhead():  \n",
    "print('\\n')\n",
    "print('---total_sendsize_list---')\n",
    "total_size = 0\n",
    "for size in total_sendsize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "print(\"total_sendsize size: {} bytes\".format(total_size))\n",
    "print(\"number of total_send: \", len(total_sendsize_list))\n",
    "print('\\n')\n",
    "\n",
    "print('---total_receivesize_list---')\n",
    "total_size = 0\n",
    "for size in total_receivesize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "print(\"total receive sizes: {} bytes\".format(total_size) )\n",
    "print(\"number of total receive: \", len(total_receivesize_list) )\n",
    "print('\\n')\n",
    "\n",
    "for i in range(users):\n",
    "    print('---client_sendsize_list(user{})---'.format(i))\n",
    "    total_size = 0\n",
    "    for size in client_sendsize_list[i]:\n",
    "#         print(size)\n",
    "        total_size += size\n",
    "    print(\"total client_sendsizes(user{}): {} bytes\".format(i, total_size))\n",
    "    print(\"number of client_send(user{}): \".format(i), len(client_sendsize_list[i]))\n",
    "    print('\\n')\n",
    "\n",
    "    print('---client_receivesize_list(user{})---'.format(i))\n",
    "    total_size = 0\n",
    "    for size in client_receivesize_list[i]:\n",
    "#         print(size)\n",
    "        total_size += size\n",
    "    print(\"total client_receive sizes(user{}): {} bytes\".format(i, total_size))\n",
    "    print(\"number of client_send(user{}): \".format(i), len(client_receivesize_list[i]))\n",
    "    print('\\n')\n",
    "\n",
    "print('---train_sendsize_list---')\n",
    "total_size = 0\n",
    "for size in train_sendsize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "print(\"total train_sendsizes: {} bytes\".format(total_size))\n",
    "print(\"number of train_send: \", len(train_sendsize_list) )\n",
    "print('\\n')\n",
    "\n",
    "print('---train_receivesize_list---')\n",
    "total_size = 0\n",
    "for size in train_receivesize_list:\n",
    "#     print(size)\n",
    "    total_size += size\n",
    "print(\"total train_receivesizes: {} bytes\".format(total_size))\n",
    "print(\"number of train_receive: \", len(train_receivesize_list) )\n",
    "print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of train and each of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8529\n",
      "Precision: 0.8310\n",
      "Recall: 0.7308\n",
      "F1: 0.7622\n",
      "Confusion Matrix: \n",
      "tensor([[ 4,  4],\n",
      "        [ 1, 25]])\n",
      "Train Accuracy: 83.33%, Train Loss: 0.3948\n",
      "Accuracy: 0.8397\n",
      "Precision: 0.8398\n",
      "Recall: 0.7325\n",
      "F1: 0.7624\n",
      "Confusion Matrix: \n",
      "tensor([[ 21,  21],\n",
      "        [  4, 110]])\n",
      "Test Accuracy: 83.97%, Test Loss: 0.3302\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on the train set\n",
    "with torch.no_grad():\n",
    "    train_accuracy, train_loss = calculate_performance(sq_model, dataloaders['train'], criterion, True)\n",
    "    print(\"Train Accuracy: {:.2f}%, Train Loss: {:.4f}\".format(train_accuracy * 100, train_loss))\n",
    "\n",
    "# Evaluation on the test set\n",
    "with torch.no_grad():\n",
    "    test_accuracy, test_loss = calculate_performance(sq_model, dataloaders['test'], criterion, True)\n",
    "    print(\"Test Accuracy: {:.2f}%, Test Loss: {:.4f}\".format(test_accuracy * 100, test_loss))\n",
    "\n",
    "\n",
    "# Save the trained model\n",
    "PATH = './'+dataset_name+'_fd_SqueezeNet.pth'\n",
    "torch.save(sq_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot the training and validation accuracy\n",
    "# plt_dir = './plots'\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plt.plot(train_acc_list, label='Training Accuracy')\n",
    "# plt.plot(val_acc_list, label='Validation Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.title('Training and Validation Accuracy')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# # plt.show()\n",
    "# plt.savefig(plt_dir+'/accuracy_curves/'+ f'acc_{dataset_name}_{sq_model.__class__.__name__}.png')\n",
    "\n",
    "# #plot the training and validation loss\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plt.plot(train_loss_list, label='Training Loss')\n",
    "# plt.plot(val_loss_list, label='Validation Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training and Validation Loss')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# # plt.show()\n",
    "# plt.savefig(plt_dir+'/loss_curves/'+ f'loss_{dataset_name}_{sq_model.__class__.__name__}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List saved successfully to data.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def save_list_to_csv(data, filename):\n",
    "  \"\"\"Saves all values in a list to a CSV file.\n",
    "\n",
    "  Args:\n",
    "      data: The list containing the values to be saved.\n",
    "      filename: The name of the CSV file to create.\n",
    "  \"\"\"\n",
    "\n",
    "  # Open the CSV file in write mode with proper newline handling\n",
    "  with open(filename, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "\n",
    "    # Write each value in the list to a separate row\n",
    "    for item in data:\n",
    "      csv_writer.writerow([item])  # Wrap in a list for proper formatting\n",
    "\n",
    "\n",
    "# Save the list to a CSV file\n",
    "save_list_to_csv(train_acc_list, \"train_acc.csv\")\n",
    "save_list_to_csv(val_acc_list, \"val_acc.csv\")\n",
    "save_list_to_csv(train_loss_list, \"train_loss.csv\")\n",
    "save_list_to_csv(val_loss_list, \"val_loss.csv\")\n",
    "print(\"List saved successfully to data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
